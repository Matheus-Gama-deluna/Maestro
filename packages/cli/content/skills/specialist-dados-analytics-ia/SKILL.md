---
name: specialist-dados-analytics-ia
description: Pipelines ETL, dashboards e m√©tricas inteligentes.
allowed-tools: Read, Write, Edit, Glob, Grep
---

# Dados e Analytics com IA ¬∑ Skill do Especialista

## üéØ Miss√£o
Projetar e implementar pipelines de dados e dashboards guiados por IA, transformando dados brutos em insights acion√°veis.

## üß≠ Quando ativar
- Fase: Fase 13 ¬∑ Dados
- Workflows recomendados: /nova-feature, /maestro
- Use quando o projeto precisa de m√©tricas acion√°veis e automa√ß√£o de dados.

## üì• Inputs obrigat√≥rios
- Requisitos de neg√≥cio e KPIs
- Fontes de dados dispon√≠veis
- Modelo de dom√≠nio (`docs/04-modelo/modelo-dominio.md`)
- Regras de privacidade e compliance
- CONTEXTO.md do projeto

## üì§ Outputs gerados
- ETL pipelines documentados
- Modelagem dimensional completa
- Dashboards e m√©tricas operacionais
- Testes de qualidade de dados
- Documenta√ß√£o de schemas

## ‚úÖ Quality Gate
- ETL funcionando com monitoramento
- Dashboards acess√≠veis e atualizados
- M√©tricas coletadas e monitoradas
- Qualidade de dados validada
- SLAs definidos e cumpridos
- Documenta√ß√£o completa

## üîß Ferramentas Recomendadas

### Orquestra√ß√£o
- **Airflow**: DAGs em Python, escal√°vel
- **Dagster**: orientado a assets
- **Prefect**: pipelines modernas
- **Apache NiFi**: fluxos visuais

### Transforma√ß√£o
- **dbt**: SQL-first, testes, documenta√ß√£o
- **Spark**: processamento distribu√≠do
- **Pandas**: an√°lise local
- **Great Expectations**: valida√ß√£o de dados

### Armazenamento
- **PostgreSQL/MySQL**: dados transacionais
- **BigQuery/Redshift/Snowflake**: data warehouse
- **S3/GCS**: data lake
- **ClickHouse**: analytics em tempo real

### Visualiza√ß√£o
- **Metabase**: open-source, f√°cil de usar
- **Looker/Tableau**: enterprise
- **Streamlit**: dashboards em Python
- **Grafana**: m√©tricas e alertas

## ÔøΩ Processo Obrigat√≥rio de Analytics

### 1. An√°lise de Requisitos
```text
Com base nos requisitos de neg√≥cio:
[COLE REQUISITOS]

Identifique:
- KPIs cr√≠ticos do neg√≥cio
- M√©tricas de sucesso
- Fontes de dados dispon√≠veis
- Frequ√™ncia de atualiza√ß√£o necess√°ria
- Regras de privacidade e compliance
```

### 2. Modelagem Dimensional
```text
Contexto de neg√≥cio:
[COLE MODELO DE DOM√çNIO]

Proponha um modelo dimensional com:
- Tabelas de fato (m√©tricas)
- Tabelas de dimens√£o
- Granularidade de cada fato
- Slowly Changing Dimensions (SCD)
- Relacionamentos e chaves
```

### 3. Pipeline ETL/ELT
```text
Preciso criar um pipeline com:
- Fonte: [API REST, PostgreSQL, arquivos]
- Destino: [BigQuery, Redshift, Snowflake]
- Frequ√™ncia: [di√°ria, hor√°ria, real-time]
- Transforma√ß√µes necess√°rias

Gere c√≥digo usando [FERRAMENTA] com:
- Extra√ß√£o com valida√ß√£o
- Transforma√ß√£o com limpeza
- Carga incremental
- Testes de qualidade
```

### 4. Dashboards e Visualiza√ß√£o
```text
Para as m√©tricas definidas:
[COLE M√âTRICAS]

Crie dashboards com:
- KPIs principais
- Filtros interativos
- Visualiza√ß√µes adequadas
- Alertas configurados
- Acesso controlado
```

## üìã Checklists Obrigat√≥rias

### Pipeline de Dados
- [ ] Fonte de dados documentada
- [ ] Schema de entrada validado
- [ ] Transforma√ß√µes testadas
- [ ] Idempot√™ncia garantida (reruns seguros)
- [ ] Monitoramento de falhas
- [ ] SLA definido e monitorado
- [ ] Logs estruturados

### Qualidade de Dados
- [ ] Testes de nulidade em campos obrigat√≥rios
- [ ] Testes de unicidade em chaves
- [ ] Testes de integridade referencial
- [ ] Freshness (dados atualizados)
- [ ] Documenta√ß√£o de campos
- [ ] Perfis de dados atualizados

### Modelagem Dimensional
- [ ] Fatos e dimens√µes identificadas
- [ ] Granularidade definida
- [ ] Slowly Changing Dimensions (SCD) planejadas
- [ ] Surrogate keys implementadas
- [ ] √çndices otimizados para queries
- [ ] Particionamento estrat√©gico

## üö® Guardrails Cr√≠ticos

### ‚ùå NUNCA Fa√ßa
- **NUNCA** exponha dados sens√≠veis sem anonimiza√ß√£o
- **NUNCA** ignore SLAs de dados
- **NUNCA** pule valida√ß√£o de qualidade
- **NUNCA** use dados sem governan√ßa

### ‚úÖ SEMPRE Fa√ßa
- **SEMPRE** documente schemas e transforma√ß√µes
- **SEMPRE** implemente testes automatizados
- **SEMPRE** monitore performance dos pipelines
- **SEMPRE** respeite regras de privacidade

### üîê Governan√ßa de Dados Obrigat√≥ria
```yaml
# Exemplo de regras de privacidade
data_governance:
  privacy:
    - anonymize_pii: true
    - retention_policy: 365_days
    - access_control: rbac
  quality:
    - null_checks: mandatory
    - duplicate_detection: true
    - freshness_threshold: 24h
  security:
    - encryption_at_rest: true
    - audit_logs: enabled
    - access_monitoring: true
```

## üîÑ Context Flow

### Artefatos Obrigat√≥rios para Iniciar
Cole no in√≠cio:
1. Requisitos de neg√≥cio com KPIs
2. Modelo de dom√≠nio com entidades
3. Fontes de dados dispon√≠veis
4. CONTEXTO.md com restri√ß√µes
5. Regras de compliance (se aplic√°vel)

### Prompt de Continua√ß√£o
```
Atue como Engenheiro de Dados S√™nior.

Contexto do projeto:
[COLE docs/CONTEXTO.md]

Modelo de dom√≠nio:
[COLE docs/04-modelo/modelo-dominio.md]

Requisitos de neg√≥cio:
[COLE REQUISITOS COM KPIS]

Preciso [modelar dados / criar pipeline / definir m√©tricas].
```

### Ao Concluir Esta Fase
1. **Implemente pipelines** ETL/ELT
2. **Crie modelos** dimensionais
3. **Configure dashboards** e alertas
4. **Implemente testes** de qualidade
5. **Documente schemas** e transforma√ß√µes
6. **Monitore SLAs** e performance

## üìä M√©tricas e KPIs

### Indicadores de Pipeline
- **Latency:** < 30 minutos para dados frescos
- **Throughput:** > 1000 registros/segundo
- **Reliability:** > 99.5% uptime
- **Data Quality:** > 95% sem erros

### KPIs de Neg√≥cio (Exemplos)
- **E-commerce:** Taxa de convers√£o, valor m√©dio pedido
- **SaaS:** MRR, churn rate, LTV
- **M√≠dia:** Page views, tempo de sess√£o, engajamento

## üìã Templates Prontos

### Modelo dbt (Star Schema)
```sql
-- models/marts/fct_orders.sql
{{ config(materialized='incremental', unique_key='order_id') }}

SELECT
    o.id AS order_id,
    o.customer_id,
    o.created_at,
    o.total_amount,
    c.name AS customer_name,
    c.segment AS customer_segment
FROM {{ ref('stg_orders') }} o
LEFT JOIN {{ ref('dim_customers') }} c ON o.customer_id = c.customer_id

{% if is_incremental() %}
WHERE o.created_at > (SELECT MAX(created_at) FROM {{ this }})
{% endif %}
```

### Teste dbt (schema.yml)
```yaml
version: 2
models:
  - name: fct_orders
    description: Tabela de fatos de pedidos
    columns:
      - name: order_id
        tests:
          - unique
          - not_null
      - name: customer_id
        tests:
          - not_null
          - relationships:
              to: ref('dim_customers')
              field: customer_id
```

### Pipeline Airflow
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def extract_orders():
    # Extra√ß√£o da fonte de dados
    pass

def transform_orders():
    # Transforma√ß√£o e limpeza
    pass

def load_orders():
    # Carga no data warehouse
    pass

with DAG(
    dag_id='orders_pipeline',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False
) as dag:
    extract = PythonOperator(
        task_id='extract_orders',
        python_callable=extract_orders
    )
    
    transform = PythonOperator(
        task_id='transform_orders',
        python_callable=transform_orders
    )
    
    load = PythonOperator(
        task_id='load_orders',
        python_callable=load_orders
    )
    
    extract >> transform >> load
```

## ÔøΩüîó Skills complementares
- `database-design`
- `performance-profiling`
- `sql-optimization`
- `data-visualization`

## üìÇ Refer√™ncias essenciais
- **Especialista original:** `content/specialists/Especialista em Dados e Analytics com IA.md`
- **Artefatos alvo:**
  - ETL pipelines documentados
  - Modelagem dimensional completa
  - Dashboards e m√©tricas operacionais
  - Testes de qualidade de dados