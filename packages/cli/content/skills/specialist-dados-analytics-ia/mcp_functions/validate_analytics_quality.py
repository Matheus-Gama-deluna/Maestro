#!/usr/bin/env python3
"""
Fun√ß√£o MCP de Refer√™ncia: validate_analytics_quality

Esta fun√ß√£o serve como refer√™ncia para implementa√ß√£o no MCP.
N√ÉO EXECUT√ÅVEL LOCALMENTE - Apenas documenta√ß√£o da estrutura esperada.

Prop√≥sito: Validar qualidade do pipeline de analytics
"""

async def validate_analytics_quality(params: dict) -> dict:
    """
    Valida qualidade do pipeline de analytics
    
    Args:
        params: {
            "pipeline_path": str,                 # Caminho do pipeline
            "feature_content": dict,               # Conte√∫do da feature
            "validation_level": str,               # basic|complete|strict
            "data_quality_checks": bool,           # Validar qualidade de dados
            "pipeline_tests": bool,                 # Testar pipeline
            "dashboard_validation": bool,           # Validar dashboard
            "project_context": dict              # Contexto do projeto
        }
    
    Returns:
        dict: {
            "success": bool,                      # Status da valida√ß√£o
            "score": int,                         # Score 0-100
            "issues": list[dict],                 # Issues encontrados
            "recommendations": list[str],         # Recomenda√ß√µes
            "can_proceed": bool,                  # Pode prosseguir
            "validation_details": dict,           # Detalhes da valida√ß√£o
            "quality_metrics": dict,              # M√©tricas de qualidade
            "compliance_status": dict             # Status de compliance
        }
    """
    
    # IMPLEMENTA√á√ÉO ESPERADA NO MCP:
    
    feature_content = params["feature_content"]
    validation_level = params.get("validation_level", "complete")
    
    # 1. Valida√ß√£o Estrutural
    structure_validation = await validate_analytics_structure(feature_content)
    
    # 2. Valida√ß√£o de Conte√∫do
    content_validation = await validate_analytics_content(feature_content)
    
    # 3. Valida√ß√£o de L√≥gica
    logic_validation = await validate_analytics_logic(feature_content)
    
    # 4. Valida√ß√£o de Qualidade de Dados (se solicitado)
    data_quality_validation = {}
    if params.get("data_quality_checks", True):
        data_quality_validation = await validate_data_quality(feature_content)
    
    # 5. Valida√ß√£o de Pipeline (se solicitado)
    pipeline_validation = {}
    if params.get("pipeline_tests", True):
        pipeline_validation = await validate_pipeline_implementation(feature_content)
    
    # 6. Valida√ß√£o de Dashboard (se solicitado)
    dashboard_validation = {}
    if params.get("dashboard_validation", True):
        dashboard_validation = await validate_dashboard_implementation(feature_content)
    
    # 7. Calcular score final
    final_score = calculate_analytics_final_score(
        structure_validation,
        content_validation,
        logic_validation,
        data_quality_validation,
        pipeline_validation,
        dashboard_validation
    )
    
    # 8. Gerar issues
    issues = generate_analytics_issues(
        structure_validation,
        content_validation,
        logic_validation,
        data_quality_validation,
        pipeline_validation,
        dashboard_validation
    )
    
    # 9. Gerar recomenda√ß√µes
    recommendations = generate_analytics_recommendations(
        feature_content,
        final_score,
        issues
    )
    
    # 10. Determinar se pode prosseguir
    can_proceed = determine_analytics_proceed_status(final_score, validation_level, issues)
    
    return {
        "success": True,
        "score": final_score,
        "issues": issues,
        "recommendations": recommendations,
        "can_proceed": can_proceed,
        "validation_details": {
            "structure": structure_validation,
            "content": content_validation,
            "logic": logic_validation
        },
        "quality_metrics": data_quality_validation,
        "pipeline_status": pipeline_validation,
        "dashboard_status": dashboard_validation
    }


# FUN√á√ïES DE VALIDA√á√ÉO

async def validate_analytics_structure(feature_content: dict) -> dict:
    """
    Valida estrutura da feature de analytics
    
    Args:
        feature_content: Conte√∫do da feature
        
    Returns:
        dict: Resultado da valida√ß√£o estrutural
    """
    result = {
        "valid": True,
        "score": 0,
        "errors": [],
        "warnings": []
    }
    
    # Campos obrigat√≥rios
    required_fields = [
        "project_name",
        "business_objective",
        "data_sources",
        "kpi_definitions",
        "pipeline_config",
        "data_model"
    ]
    
    for field in required_fields:
        if not feature_content.get(field):
            result["errors"].append(f"Campo obrigat√≥rio ausente: {field}")
            result["valid"] = False
    
    # Validar tipos
    type_validations = {
        "data_sources": list,
        "kpi_definitions": list,
        "pipeline_config": dict,
        "data_model": dict,
        "dashboard_config": dict
    }
    
    for field, expected_type in type_validations.items():
        value = feature_content.get(field)
        if value is not None and not isinstance(value, expected_type):
            result["errors"].append(f"Tipo inv√°lido em {field}: esperado {expected_type.__name__}")
            result["valid"] = False
    
    # Validar estrutura de fontes de dados
    data_sources = feature_content.get("data_sources", [])
    for i, source in enumerate(data_sources):
        if not isinstance(source, dict):
            result["errors"].append(f"Fonte de dados {i} deve ser um dicion√°rio")
            result["valid"] = False
            continue
        
        required_source_fields = ["name", "type", "connection"]
        for field in required_source_fields:
            if not source.get(field):
                result["errors"].append(f"Fonte {i}: campo obrigat√≥rio ausente: {field}")
                result["valid"] = False
    
    # Validar estrutura de KPIs
    kpi_definitions = feature_content.get("kpi_definitions", [])
    for i, kpi in enumerate(kpi_definitions):
        if not isinstance(kpi, dict):
            result["errors"].append(f"KPI {i} deve ser um dicion√°rio")
            result["valid"] = False
            continue
        
        required_kpi_fields = ["name", "formula", "target", "frequency"]
        for field in required_kpi_fields:
            if not kpi.get(field):
                result["errors"].append(f"KPI {i}: campo obrigat√≥rio ausente: {field}")
                result["valid"] = False
    
    # Calcular score estrutural
    if result["valid"]:
        result["score"] = 100
    else:
        result["score"] = max(0, 100 - (len(result["errors"]) * 10))
    
    return result


async def validate_analytics_content(feature_content: dict) -> dict:
    """
    Valida conte√∫do da feature de analytics
    
    Args:
        feature_content: Conte√∫do da feature
        
    Returns:
        dict: Resultado da valida√ß√£o de conte√∫do
    """
    result = {
        "valid": True,
        "score": 0,
        "errors": [],
        "warnings": []
    }
    
    # Valores permitidos (enums)
    enum_validations = {
        "data_sources.type": ["Database", "API", "File", "Stream"],
        "kpi_definitions.frequency": ["Real-time", "Di√°rio", "Semanal", "Mensal"],
        "pipeline_config.orchestration": ["Airflow", "Dagster", "Prefect"],
        "data_model.type": ["Star Schema", "Snowflake", "Data Vault"],
        "dashboard_config.tool": ["Metabase", "Looker", "Tableau", "Grafana"]
    }
    
    for field, allowed_values in enum_validations.items():
        value = get_nested_analytics_value(feature_content, field)
        if value and value not in allowed_values:
            result["errors"].append(f"Valor inv√°lido em {field}: {value}. Permitidos: {allowed_values}")
            result["valid"] = False
    
    # Valida√ß√µes de formato
    format_validations = {
        "project_name": r"^[a-zA-Z0-9\s\-_]+$",
        "pipeline_config.schedule": r"^(@daily|@hourly|@weekly|\d+\s+\*+\s+\*)$"
    }
    
    import re
    for field, pattern in format_validations.items():
        value = feature_content.get(field)
        if value and not re.match(pattern, value):
            result["errors"].append(f"Formato inv√°lido em {field}: {value}")
            result["valid"] = False
    
    # Valida√ß√µes de l√≥gica de neg√≥cio
    business_validations = [
        {
            "condition": lambda t: len(t.get("data_sources", [])) == 0,
            "error": "Pelo menos uma fonte de dados deve ser configurada",
            "field": "data_sources"
        },
        {
            "condition": lambda t: len(t.get("kpi_definitions", [])) == 0,
            "error": "Pelo menos um KPI deve ser definido",
            "field": "kpi_definitions"
        },
        {
            "condition": lambda t: t.get("pipeline_config", {}).get("orchestration") and not t.get("pipeline_config", {}).get("schedule"),
            "error": "Pipeline com orquestra√ß√£o requer schedule definido",
            "field": "pipeline_config.schedule"
        }
    ]
    
    for validation in business_validations:
        if validation["condition"](feature_content):
            result["errors"].append(validation["error"])
            result["valid"] = False
    
    # Calcular score de conte√∫do
    total_fields = len(enum_validations) + len(format_validations) + len(business_validations)
    valid_fields = total_fields - len(result["errors"])
    result["score"] = int((valid_fields / total_fields) * 100) if total_fields > 0 else 0
    
    return result


async def validate_analytics_logic(feature_content: dict) -> dict:
    """
    Valida l√≥gica de neg√≥cio da feature de analytics
    
    Args:
        feature_content: Conte√∫do da feature
        
    Returns:
        dict: Resultado da valida√ß√£o l√≥gica
    """
    result = {
        "valid": True,
        "score": 0,
        "errors": [],
        "warnings": []
    }
    
    # Regras de consist√™ncia
    consistency_rules = [
        {
            "name": "Fontes e KPIs Consistentes",
            "check": lambda t: (
                len(t.get("data_sources", [])) > 0 and
                len(t.get("kpi_definitions", [])) > 0
            ),
            "error": "Fontes de dados e KPIs devem ser configurados"
        },
        {
            "name": "Pipeline Completo",
            "check": lambda t: (
                t.get("pipeline_config", {}).get("extract") and
                t.get("pipeline_config", {}).get("transform") and
                t.get("pipeline_config", {}).get("load")
            ),
            "error": "Pipeline deve ter extract, transform e load"
        },
        {
            "name": "Modelo Dimensional Definido",
            "check": lambda t: (
                t.get("data_model", {}).get("type") and
                t.get("data_model", {}).get("fact_tables") and
                t.get("data_model", {}).get("dimensions")
            ),
            "error": "Modelo dimensional deve ter tipo, fatos e dimens√µes"
        },
        {
            "name": "Dashboard Configurado",
            "check": lambda t: (
                t.get("dashboard_config", {}).get("tool") and
                t.get("dashboard_config", {}).get("kpi_mappings")
            ),
            "error": "Dashboard deve ter ferramenta e mapeamento de KPIs"
        }
    ]
    
    for rule in consistency_rules:
        if not rule["check"](feature_content):
            result["errors"].append(f"{rule['name']}: {rule['error']}")
            result["valid"] = False
    
    # Regras de best practices
    best_practices = [
        {
            "name": "M√∫ltiplas Fontes de Dados",
            "check": lambda t: len(t.get("data_sources", [])) >= 2,
            "warning": "Recomendado usar m√∫ltiplas fontes para analytics robusto"
        },
        {
            "name": "KPIs com Metas",
            "check": lambda t: all(
                kpi.get("target") for kpi in t.get("kpi_definitions", [])
            ),
            "warning": "Todos os KPIs devem ter metas definidas"
        },
        {
            "name": "Monitoramento Configurado",
            "check": lambda t: t.get("monitoring_config", {}).get("enabled", False),
            "warning": "Recomendado habilitar monitoramento do pipeline"
        },
        {
            "name": "Alertas Configurados",
            "check": lambda t: t.get("alerting_config", {}).get("enabled", False),
            "warning": "Recomendado configurar alertas para KPIs cr√≠ticos"
        }
    ]
    
    for practice in best_practices:
        if not practice["check"](feature_content):
            result["warnings"].append(f"{practice['name']}: {practice['warning']}")
    
    # Calcular score l√≥gico
    total_rules = len(consistency_rules) + len(best_practices)
    passed_rules = sum([
        rule["check"](feature_content) for rule in consistency_rules
    ]) + sum([
        practice["check"](feature_content) for practice in best_practices
    ])
    
    result["score"] = int((passed_rules / total_rules) * 100) if total_rules > 0 else 0
    
    return result


async def validate_data_quality(feature_content: dict) -> dict:
    """
    Valida aspectos de qualidade de dados
    
    Args:
        feature_content: Conte√∫do da feature
        
    Returns:
        dict: Resultado da valida√ß√£o de qualidade de dados
    """
    result = {
        "valid": True,
        "score": 0,
        "issues": [],
        "recommendations": []
    }
    
    # Data quality checks
    quality_checks = [
        {
            "name": "Data Completeness",
            "check": lambda t: t.get("data_quality", {}).get("completeness_threshold", 0) >= 95,
            "issue": "Threshold de completude muito baixo (< 95%)",
            "recommendation": "Configure threshold de completude ‚â• 95%"
        },
        {
            "name": "Data Accuracy",
            "check": lambda t: t.get("data_quality", {}).get("accuracy_threshold", 0) >= 99,
            "issue": "Threshold de acur√°cia muito baixo (< 99%)",
            "recommendation": "Configure threshold de acur√°cia ‚â• 99%"
        },
        {
            "name": "Data Freshness",
            "check": lambda t: t.get("data_quality", {}).get("freshness_hours", 24) <= 1,
            "issue": "Freshness muito baixo (> 1 hora)",
            "recommendation": "Configure freshness ‚â§ 1 hora para dados em tempo real"
        },
        {
            "name": "Data Consistency",
            "check": lambda t: t.get("data_quality", {}).get("consistency_checks", False),
            "issue": "Checks de consist√™ncia n√£o configurados",
            "recommendation": "Implemente checks de consist√™ncia automatizados"
        }
    ]
    
    for check in quality_checks:
        if not check["check"](feature_content):
            result["issues"].append({
                "type": check["name"],
                "description": check["issue"],
                "recommendation": check["recommendation"],
                "severity": "high"
            })
            result["valid"] = False
        else:
            result["recommendations"].append(f"‚úÖ {check['name']} configurado")
    
    # Calcular score de qualidade
    total_checks = len(quality_checks)
    passed_checks = sum([check["check"](feature_content) for check in quality_checks])
    result["score"] = int((passed_checks / total_checks) * 100) if total_checks > 0 else 0
    
    return result


async def validate_pipeline_implementation(feature_content: dict) -> dict:
    """
    Valida implementa√ß√£o do pipeline
    
    Args:
        feature_content: Conte√∫do da feature
        
    Returns:
        dict: Resultado da valida√ß√£o do pipeline
    """
    result = {
        "valid": True,
        "score": 0,
        "metrics": {},
        "recommendations": []
    }
    
    # Pipeline implementation checks
    pipeline_checks = [
        {
            "name": "Orchestration Tool",
            "check": lambda t: t.get("pipeline_config", {}).get("orchestration") in ["Airflow", "Dagster", "Prefect"],
            "recommendation": "Configure ferramenta de orquestra√ß√£o (Airflow/Dagster/Prefect)"
        },
        {
            "name": "Schedule Defined",
            "check": lambda t: t.get("pipeline_config", {}).get("schedule"),
            "recommendation": "Defina schedule para execu√ß√£o do pipeline"
        },
        {
            "name": "Error Handling",
            "check": lambda t: t.get("pipeline_config", {}).get("error_handling", False),
            "recommendation": "Implemente tratamento de erros robusto"
        },
        {
            "name": "Retry Logic",
            "check": lambda t: t.get("pipeline_config", {}).get("retry_config", {}).get("enabled", False),
            "recommendation": "Configure retry logic para falhas tempor√°rias"
        },
        {
            "name": "Monitoring Enabled",
            "check": lambda t: t.get("pipeline_config", {}).get("monitoring", {}).get("enabled", False),
            "recommendation": "Habilite monitoramento do pipeline"
        }
    ]
    
    for check in pipeline_checks:
        if check["check"](feature_content):
            result["recommendations"].append(f"‚úÖ {check['name']} configurado")
        else:
            result["recommendations"].append(f"‚ö†Ô∏è {check['recommendation']}")
    
    # Simular m√©tricas de pipeline
    result["metrics"] = {
        "expected_latency": "< 30 minutos",
        "expected_throughput": "> 1000 registros/segundo",
        "expected_availability": "99.5%",
        "expected_recovery_time": "< 5 minutos"
    }
    
    # Calcular score de pipeline
    total_checks = len(pipeline_checks)
    configured_checks = sum([check["check"](feature_content) for check in pipeline_checks])
    result["score"] = int((configured_checks / total_checks) * 100) if total_checks > 0 else 0
    
    return result


async def validate_dashboard_implementation(feature_content: dict) -> dict:
    """
    Valida implementa√ß√£o do dashboard
    
    Args:
        feature_content: Conte√∫do da feature
        
    Returns:
        dict: Resultado da valida√ß√£o do dashboard
    """
    result = {
        "valid": True,
        "score": 0,
        "metrics": {},
        "recommendations": []
    }
    
    # Dashboard implementation checks
    dashboard_checks = [
        {
            "name": "BI Tool Selected",
            "check": lambda t: t.get("dashboard_config", {}).get("tool") in ["Metabase", "Looker", "Tableau", "Grafana"],
            "recommendation": "Selecione ferramenta de BI (Metabase/Looker/Tableau/Grafana)"
        },
        {
            "name": "KPI Mappings",
            "check": lambda t: len(t.get("dashboard_config", {}).get("kpi_mappings", [])) > 0,
            "recommendation": "Mapeie KPIs para visualiza√ß√µes no dashboard"
        },
        {
            "name": "Filters Configured",
            "check": lambda t: len(t.get("dashboard_config", {}).get("filters", [])) > 0,
            "recommendation": "Configure filtros interativos no dashboard"
        },
        {
            "name": "Access Control",
            "check": lambda t: t.get("dashboard_config", {}).get("access_control", {}).get("enabled", False),
            "recommendation": "Configure controle de acesso para o dashboard"
        },
        {
            "name": "Alerts Configured",
            "check": lambda t: len(t.get("dashboard_config", {}).get("alerts", [])) > 0,
            "recommendation": "Configure alertas para KPIs cr√≠ticos"
        }
    ]
    
    for check in dashboard_checks:
        if check["check"](feature_content):
            result["recommendations"].append(f"‚úÖ {check['name']} configurado")
        else:
            result["recommendations"].append(f"‚ö†Ô∏è {check['recommendation']}")
    
    # Simular m√©tricas de dashboard
    result["metrics"] = {
        "expected_load_time": "< 5 segundos",
        "expected_concurrent_users": "> 50",
        "expected_refresh_rate": "Real-time para KPIs cr√≠ticos",
        "expected_interactive_response": "< 1 segundo"
    }
    
    # Calcular score de dashboard
    total_checks = len(dashboard_checks)
    configured_checks = sum([check["check"](feature_content) for check in dashboard_checks])
    result["score"] = int((configured_checks / total_checks) * 100) if total_checks > 0 else 0
    
    return result


# FUN√á√ïES AUXILIARES

def get_nested_analytics_value(data: dict, path: str) -> any:
    """
    Obt√©m valor aninhado de dicion√°rio usando path notation
    
    Args:
        data: Dicion√°rio de dados
        path: Path no formato "level1.level2.level3"
        
    Returns:
        any: Valor encontrado ou None
    """
    keys = path.split('.')
    current = data
    
    for key in keys:
        if isinstance(current, dict) and key in current:
            current = current[key]
        else:
            return None
    
    return current


def calculate_analytics_final_score(*validations: dict) -> int:
    """
    Calcula score final baseado em todas as valida√ß√µes
    
    Args:
        *validations: Resultados das valida√ß√µes
        
    Returns:
        int: Score final 0-100
    """
    total_score = 0
    total_weight = 0
    
    weights = {
        "structure": 20,
        "content": 25,
        "logic": 25,
        "data_quality": 15,
        "pipeline": 10,
        "dashboard": 5
    }
    
    validation_types = ["structure", "content", "logic", "data_quality", "pipeline", "dashboard"]
    
    for i, validation in enumerate(validations):
        if validation and "score" in validation:
            weight = weights.get(validation_types[i], 10)
            total_score += validation["score"] * weight
            total_weight += weight
    
    return int(total_score / total_weight) if total_weight > 0 else 0


def generate_analytics_issues(*validations: dict) -> list[dict]:
    """
    Gera lista consolidada de issues
    
    Args:
        *validations: Resultados das valida√ß√µes
        
    Returns:
        list[dict]: Lista de issues
    """
    all_issues = []
    
    for validation in validations:
        if validation:
            if "errors" in validation:
                for error in validation["errors"]:
                    all_issues.append({
                        "type": "error",
                        "description": error,
                        "severity": "high"
                    })
            
            if "issues" in validation:
                all_issues.extend(validation["issues"])
    
    return all_issues


def generate_analytics_recommendations(feature_content: dict, score: int, issues: list[dict]) -> list[str]:
    """
    Gera recomenda√ß√µes baseadas no template e issues
    
    Args:
        feature_content: Conte√∫do do template
        score: Score final
        issues: Issues encontrados
        
    Returns:
        list[str]: Lista de recomenda√ß√µes
    """
    recommendations = []
    
    # Recomenda√ß√µes baseadas no score
    if score < 60:
        recommendations.append("‚ö†Ô∏è Score baixo (< 60). Reconfigura√ß√£o obrigat√≥ria recomendada.")
    elif score < 80:
        recommendations.append("‚ö†Ô∏è Score m√©dio (60-79). Revisar e corrigir issues cr√≠ticos.")
    else:
        recommendations.append("‚úÖ Score bom (‚â• 80). Configura√ß√£o aprovada.")
    
    # Recomenda√ß√µes baseadas em issues
    for issue in issues:
        if issue["severity"] == "high":
            recommendations.append(f"üî¥ CR√çTICO: {issue['description']}")
    
    # Recomenda√ß√µes espec√≠ficas
    if not feature_content.get("data_sources"):
        recommendations.append("üîó Mapeie todas as fontes de dados dispon√≠veis")
    
    if not feature_content.get("kpi_definitions"):
        recommendations.append("üìä Defina KPIs claros e mensur√°veis")
    
    if not feature_content.get("pipeline_config"):
        recommendations.append("üîÑ Implemente pipeline ETL/ELT completo")
    
    if not feature_content.get("dashboard_config"):
        recommendations.append("üìà Configure dashboards interativos")
    
    return recommendations


def determine_analytics_proceed_status(score: int, validation_level: str, issues: list[dict]) -> bool:
    """
    Determina se pode prosseguir baseado no score e n√≠vel de valida√ß√£o
    
    Args:
        score: Score final
        validation_level: N√≠vel de valida√ß√£o (basic|complete|strict)
        issues: Issues encontrados
        
    Returns:
        bool: Pode prosseguir
    """
    # Thresholds por n√≠vel
    thresholds = {
        "basic": 60,
        "complete": 80,
        "strict": 90
    }
    
    threshold = thresholds.get(validation_level, 80)
    
    # Verificar score m√≠nimo
    if score < threshold:
        return False
    
    # Verificar issues cr√≠ticos
    critical_issues = [issue for issue in issues if issue.get("severity") == "high"]
    if critical_issues and validation_level == "strict":
        return False
    
    return True


# EXEMPLO DE USO

if __name__ == "__main__":
    """
    Exemplo de como a fun√ß√£o seria chamada no MCP:
    
    result = await validate_analytics_quality({
        "pipeline_path": "analytics/dashboard-analytics/",
        "feature_content": feature_json,
        "validation_level": "complete",
        "data_quality_checks": True,
        "pipeline_tests": True,
        "dashboard_validation": True,
        "project_context": {
            "industry": "e-commerce",
            "data_volume": "high",
            "real_time_requirements": True
        }
    })
    
    print(f"Validation Success: {result['success']}")
    print(f"Final Score: {result['score']}")
    print(f"Can Proceed: {result['can_proceed']}")
    print(f"Issues Found: {len(result['issues'])}")
    """
    
    print("Esta √© uma fun√ß√£o de refer√™ncia para implementa√ß√£o no MCP.")
    print("N√ÉO EXECUT√ÅVEL LOCALMENTE.")
    print("Consulte MCP_INTEGRATION.md para detalhes de implementa√ß√£o.")
