"""
Fun√ß√£o MCP de Refer√™ncia: process_frontend_to_next_phase()

ESTE ARQUIVO √â APENAS REFER√äNCIA PARA IMPLEMENTA√á√ÉO NO MCP
N√ÉO EXECUT√ÅVEL LOCALMENTE

Implementa√ß√£o real deve ser feita no servidor MCP externo.
"""

import os
import json
from typing import Dict, List, Any, Optional
from pathlib import Path

async def process_frontend_to_next_phase(params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Processa artefatos frontend para pr√≥xima fase (Deploy)
    
    Args:
        params: Dicion√°rio com par√¢metros de processamento
            - project_path: str - Caminho do projeto
            - current_phase: str - Fase atual
            - next_phase: str - Pr√≥xima fase
            - artifacts: list - Lista de artefatos gerados
            - validation_score: int - Score de valida√ß√£o
            - auto_advance: bool - Avan√ßo autom√°tico
    
    Returns:
        Dict com resultado do processamento
            - success: bool - Status da opera√ß√£o
            - processed_artifacts: list - Artefatos processados
            - context_generated: dict - Contexto gerado
            - next_phase_ready: bool - Se pr√≥xima fase est√° pronta
            - deployment_config: dict - Configura√ß√£o de deploy
            - errors: list - Lista de erros
    """
    
    try:
        project_path = params.get("project_path")
        current_phase = params.get("current_phase", "frontend")
        next_phase = params.get("next_phase", "deploy")
        validation_score = params.get("validation_score", 0)
        auto_advance = params.get("auto_advance", False)
        artifacts = params.get("artifacts", [])
        
        if not project_path:
            return {
                "success": False,
                "processed_artifacts": [],
                "context_generated": {},
                "next_phase_ready": False,
                "deployment_config": {},
                "errors": ["project_path √© obrigat√≥rio"]
            }
        
        # Verificar se pode avan√ßar
        if validation_score < 75 and not auto_advance:
            return {
                "success": False,
                "processed_artifacts": [],
                "context_generated": {},
                "next_phase_ready": False,
                "deployment_config": {},
                "errors": [f"Score de valida√ß√£o ({validation_score}) abaixo do threshold (75)"]
            }
        
        processed_artifacts = []
        
        # 1. Gerar documenta√ß√£o para deploy
        deploy_docs = await generate_deploy_documentation(project_path, validation_score)
        processed_artifacts.append(deploy_docs)
        
        # 2. Criar configura√ß√£o de deploy
        deploy_config = await create_deployment_config(project_path, next_phase, validation_score)
        processed_artifacts.append(deploy_config)
        
        # 3. Gerar contexto para pr√≥xima fase
        context = await generate_context_for_next_phase(
            project_path, current_phase, next_phase, validation_score, artifacts
        )
        
        # 4. Preparar assets para produ√ß√£o
        assets = await prepare_production_assets(project_path)
        processed_artifacts.extend(assets)
        
        # 5. Gerar scripts de deploy
        deploy_scripts = await generate_deploy_scripts(project_path, next_phase)
        processed_artifacts.extend(deploy_scripts)
        
        # 6. Criar configura√ß√£o de CI/CD
        cicd_config = await generate_cicd_config(project_path, next_phase)
        processed_artifacts.append(cicd_config)
        
        # 7. Gerar relat√≥rio de qualidade
        quality_report = await generate_quality_report(project_path, validation_score)
        processed_artifacts.append(quality_report)
        
        return {
            "success": True,
            "processed_artifacts": processed_artifacts,
            "context_generated": context,
            "next_phase_ready": True,
            "deployment_config": deploy_config,
            "errors": []
        }
        
    except Exception as e:
        return {
            "success": False,
            "processed_artifacts": [],
            "context_generated": {},
            "next_phase_ready": False,
            "deployment_config": {},
            "errors": [f"Erro no processamento: {str(e)}"]
        }

async def generate_deploy_documentation(project_path: str, validation_score: int) -> Dict[str, Any]:
    """Gera documenta√ß√£o para deploy"""
    
    deploy_info = {
        "type": "deploy_documentation",
        "content": {
            "build_commands": [
                "npm run build",
                "npm run test",
                "npm run lint"
            ],
            "environment_variables": [
                "NODE_ENV=production",
                "API_URL=https://api.example.com",
                "SENTRY_DSN=${SENTRY_DSN}",
                "ANALYTICS_ID=${ANALYTICS_ID}"
            ],
            "health_checks": [
                "/health",
                "/api/health",
                "/api/version"
            ],
            "rollback_commands": [
                "git revert HEAD",
                "npm run deploy:rollback",
                "kubectl rollout undo deployment/frontend"
            ],
            "validation_score": validation_score,
            "deployment_notes": [
                f"Score de valida√ß√£o frontend: {validation_score}/100",
                "Build otimizado para produ√ß√£o",
                "Assets minificados e com cache",
                "Testes executados com sucesso" if validation_score >= 75 else "Aten√ß√£o: Score abaixo do ideal"
            ]
        }
    }
    
    # Salvar documenta√ß√£o em arquivo
    deploy_docs_path = os.path.join(project_path, "docs", "11-deploy", "deployment-guide.md")
    os.makedirs(os.path.dirname(deploy_docs_path), exist_ok=True)
    
    docs_content = f"""# üöÄ Guia de Deploy - Frontend

## üìä Status de Qualidade
**Score de Valida√ß√£o:** {validation_score}/100  
**Status:** {"‚úÖ Aprovado" if validation_score >= 75 else "‚ö†Ô∏è Aten√ß√£o"}

## üîß Comandos de Build

### Build para Produ√ß√£o
```bash
npm run build
```

### Testes
```bash
npm run test
npm run test:coverage
```

### Lint
```bash
npm run lint
npm run lint:fix
```

## üåç Vari√°veis de Ambiente

### Obrigat√≥rias
- `NODE_ENV=production`
- `API_URL=https://api.example.com`
- `SENTRY_DSN` - Para error tracking

### Opcionais
- `ANALYTICS_ID` - Para analytics
- `FEATURE_FLAGS` - Para feature flags

## üîç Health Checks

### Endpoints de Verifica√ß√£o
- `/health` - Health check b√°sico
- `/api/health` - Health check da API
- `/api/version` - Vers√£o da aplica√ß√£o

## üîÑ Rollback

### Comandos de Rollback
```bash
# Reverter √∫ltimo commit
git revert HEAD

# Rollback do deploy
npm run deploy:rollback

# Rollback no Kubernetes
kubectl rollout undo deployment/frontend
```

## üìù Notas de Deploy

{chr(10).join(f"- {note}" for note in deploy_info["content"]["deployment_notes"])}

---

**Gerado em:** {validation_score}  
**Pr√≥xima revis√£o:** P√≥s-deploy
"""
    
    with open(deploy_docs_path, 'w', encoding='utf-8') as f:
        f.write(docs_content)
    
    deploy_info["file_path"] = deploy_docs_path
    return deploy_info

async def create_deployment_config(project_path: str, next_phase: str, validation_score: int) -> Dict[str, Any]:
    """Cria configura√ß√£o de deploy"""
    
    # Ler package.json para obter informa√ß√µes
    package_path = os.path.join(project_path, "package.json")
    app_name = "frontend-app"
    build_tool = "vite"
    
    if os.path.exists(package_path):
        try:
            with open(package_path, 'r') as f:
                package_data = json.load(f)
                app_name = package_data.get("name", "frontend-app")
                
                # Verificar build tool
                scripts = package_data.get("scripts", {})
                if "webpack" in scripts.get("build", ""):
                    build_tool = "webpack"
        except:
            pass
    
    config = {
        "type": "deployment_config",
        "phase": next_phase,
        "config": {
            "app_name": app_name,
            "build_tool": build_tool,
            "output_dir": "dist",
            "assets_dir": "assets",
            "environment": "production",
            "cdn_enabled": True,
            "cache_strategy": "cache_first",
            "validation_score": validation_score,
            "docker": {
                "enabled": True,
                "base_image": "nginx:alpine",
                "port": 80
            },
            "kubernetes": {
                "enabled": True,
                "replicas": 3,
                "resources": {
                    "requests": {
                        "cpu": "100m",
                        "memory": "128Mi"
                    },
                    "limits": {
                        "cpu": "500m",
                        "memory": "512Mi"
                    }
                }
            },
            "monitoring": {
                "enabled": True,
                "health_check": "/health",
                "metrics_endpoint": "/metrics"
            }
        }
    }
    
    # Gerar Dockerfile
    dockerfile_content = f"""FROM node:18-alpine AS builder

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

COPY . .
RUN npm run build

FROM nginx:alpine
COPY --from=builder /app/dist /usr/share/nginx/html
COPY nginx.conf /etc/nginx/nginx.conf

EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
"""
    
    dockerfile_path = os.path.join(project_path, "Dockerfile")
    with open(dockerfile_path, 'w', encoding='utf-8') as f:
        f.write(dockerfile_content)
    
    config["dockerfile_path"] = dockerfile_path
    
    # Gerar nginx.conf
    nginx_config = """events {
    worker_connections 1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css text/xml text/javascript application/javascript application/xml+rss application/json;

    # Cache static assets
    location ~* \\.(js|css|png|jpg|jpeg|gif|ico|svg)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
    }

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;

    server {
        listen 80;
        server_name localhost;
        root /usr/share/nginx/html;
        index index.html;

        # Try files and fallback to index.html for SPA
        location / {
            try_files $uri $uri/ /index.html;
        }

        # Health check
        location /health {
            access_log off;
            return 200 "healthy\\n";
            add_header Content-Type text/plain;
        }
    }
}"""
    
    nginx_path = os.path.join(project_path, "nginx.conf")
    with open(nginx_path, 'w', encoding='utf-8') as f:
        f.write(nginx_config)
    
    config["nginx_config_path"] = nginx_path
    
    return config

async def generate_context_for_next_phase(
    project_path: str, 
    current_phase: str, 
    next_phase: str, 
    validation_score: int,
    artifacts: List[str]
) -> Dict[str, Any]:
    """Gera contexto para pr√≥xima fase"""
    
    # Analisar artefatos gerados
    artifacts_info = await analyze_artifacts(project_path, artifacts)
    
    context = {
        "current_phase": current_phase,
        "next_phase": next_phase,
        "validation_score": validation_score,
        "artifacts_generated": artifacts_info["types"],
        "artifacts_count": artifacts_info["count"],
        "quality_metrics": {
            "test_coverage": await estimate_test_coverage(project_path),
            "performance_score": await estimate_performance_score(project_path),
            "accessibility_compliance": await estimate_accessibility_compliance(project_path),
            "security_score": await estimate_security_score(project_path)
        },
        "deployment_ready": validation_score >= 75,
        "recommendations": await generate_phase_recommendations(validation_score, artifacts_info),
        "dependencies": {
            "api_endpoints": await extract_api_dependencies(project_path),
            "external_services": await extract_external_dependencies(project_path),
            "environment_variables": await extract_env_variables(project_path)
        },
        "metadata": {
            "generated_at": "2026-01-29T17:00:00Z",
            "project_path": project_path,
            "phase_transition": f"{current_phase} -> {next_phase}"
        }
    }
    
    # Salvar contexto em arquivo
    context_path = os.path.join(project_path, "docs", "11-deploy", "context.json")
    os.makedirs(os.path.dirname(context_path), exist_ok=True)
    
    with open(context_path, 'w', encoding='utf-8') as f:
        json.dump(context, f, indent=2)
    
    context["context_file_path"] = context_path
    return context

async def analyze_artifacts(project_path: str, artifacts: List[str]) -> Dict[str, Any]:
    """Analisa artefatos gerados"""
    
    artifacts_info = {
        "types": [],
        "count": 0,
        "details": {}
    }
    
    # Analisar diret√≥rios src/
    src_path = os.path.join(project_path, "src")
    if os.path.exists(src_path):
        for item in os.listdir(src_path):
            item_path = os.path.join(src_path, item)
            if os.path.isdir(item_path):
                artifacts_info["types"].append(item)
                artifacts_info["details"][item] = {
                    "type": "directory",
                    "path": item_path,
                    "file_count": len([f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))])
                }
    
    # Contar arquivos principais
    file_types = {
        "components": 0,
        "pages": 0,
        "hooks": 0,
        "tests": 0,
        "types": 0
    }
    
    for root, dirs, files in os.walk(src_path):
        for file in files:
            if file.endswith(('.tsx', '.jsx', '.ts', '.js')):
                if "components" in root:
                    file_types["components"] += 1
                elif "pages" in root:
                    file_types["pages"] += 1
                elif "hooks" in root:
                    file_types["hooks"] += 1
                elif "test" in file or file.endswith(('.test.tsx', '.test.ts', '.spec.tsx', '.spec.ts')):
                    file_types["tests"] += 1
                elif "types" in root:
                    file_types["types"] += 1
    
    artifacts_info["file_types"] = file_types
    artifacts_info["count"] = sum(file_types.values())
    
    return artifacts_info

async def estimate_test_coverage(project_path: str) -> str:
    """Estima cobertura de testes"""
    
    test_files = 0
    source_files = 0
    
    src_path = os.path.join(project_path, "src")
    if os.path.exists(src_path):
        for root, dirs, files in os.walk(src_path):
            for file in files:
                if file.endswith(('.tsx', '.jsx', '.ts', '.js')):
                    if "test" in file or file.endswith(('.test.tsx', '.test.ts', '.spec.tsx', '.spec.ts')):
                        test_files += 1
                    elif not file.endswith(('.d.ts', '.config.ts', '.config.js')):
                        source_files += 1
    
    if source_files == 0:
        return "0%"
    
    coverage = min((test_files / source_files) * 100, 95)  # M√°ximo 95%
    return f"{coverage:.0f}%"

async def estimate_performance_score(project_path: str) -> str:
    """Estima score de performance"""
    
    score = 70  # Base score
    
    # Verificar otimiza√ß√µes
    src_path = os.path.join(project_path, "src")
    if os.path.exists(src_path):
        optimizations = 0
        
        for root, dirs, files in os.walk(src_path):
            for file in files:
                if file.endswith(('.tsx', '.jsx', '.ts', '.js')):
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            
                            if 'lazy(' in content or 'import(' in content:
                                optimizations += 1
                            if 'React.memo' in content or 'memo(' in content:
                                optimizations += 1
                            if 'useMemo' in content or 'useCallback' in content:
                                optimizations += 1
                    except:
                        continue
        
        score += min(optimizations * 2, 25)  # M√°ximo +25
    
    # Verificar configura√ß√£o de build
    package_path = os.path.join(project_path, "package.json")
    if os.path.exists(package_path):
        try:
            with open(package_path, 'r') as f:
                package_data = json.load(f)
                
                scripts = package_data.get("scripts", {})
                if "analyze" in scripts:
                    score += 5
                
                dev_deps = package_data.get("devDependencies", {})
                if any(dep in dev_deps for dep in ["webpack-bundle-analyzer", "@next/bundle-analyzer"]):
                    score += 5
        except:
            pass
    
    return f"{min(score, 100)}%"

async def estimate_accessibility_compliance(project_path: str) -> str:
    """Estima compliance de acessibilidade"""
    
    score = 60  # Base score
    
    src_path = os.path.join(project_path, "src")
    if os.path.exists(src_path):
        aria_files = 0
        total_files = 0
        
        for root, dirs, files in os.walk(src_path):
            for file in files:
                if file.endswith(('.tsx', '.jsx')):
                    total_files += 1
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            if 'aria-' in content or 'role=' in content:
                                aria_files += 1
                    except:
                        continue
        
        if total_files > 0:
            aria_percentage = (aria_files / total_files) * 100
            score += min(aria_percentage * 0.3, 30)  # M√°ximo +30
    
    # Verificar ferramentas de acessibilidade
    package_path = os.path.join(project_path, "package.json")
    if os.path.exists(package_path):
        try:
            with open(package_path, 'r') as f:
                package_data = json.load(f)
                
                dev_deps = package_data.get("devDependencies", {})
                if any(dep in dev_deps for dep in ["@axe-core/react", "eslint-plugin-jsx-a11y"]):
                    score += 10
        except:
            pass
    
    return f"{min(score, 100)}%"

async def estimate_security_score(project_path: str) -> str:
    """Estima score de seguran√ßa"""
    
    score = 70  # Base score
    
    # Verificar ferramentas de seguran√ßa
    package_path = os.path.join(project_path, "package.json")
    if os.path.exists(package_path):
        try:
            with open(package_path, 'r') as f:
                package_data = json.load(f)
                
                dev_deps = package_data.get("devDependencies", {})
                security_tools = ["eslint-plugin-security", "@typescript-eslint/eslint-plugin"]
                security_count = sum(1 for tool in security_tools if tool in dev_deps)
                score += security_count * 5
        except:
            pass
    
    # Verificar pr√°ticas no c√≥digo
    src_path = os.path.join(project_path, "src")
    if os.path.exists(src_path):
        security_issues = 0
        
        for root, dirs, files in os.walk(src_path):
            for file in files:
                if file.endswith(('.tsx', '.jsx', '.ts', '.js')):
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            
                            if 'innerHTML' in content and 'sanitize' not in content:
                                security_issues += 1
                            if 'eval(' in content:
                                security_issues += 1
                            if 'document.write' in content:
                                security_issues += 1
                    except:
                        continue
        
        score -= min(security_issues * 2, 20)  # Penalidade m√°xima -20
    
    return f"{max(min(score, 100), 0)}%"

async def extract_api_dependencies(project_path: str) -> List[str]:
    """Extrai depend√™ncias de API"""
    
    endpoints = set()
    
    src_path = os.path.join(project_path, "src")
    if os.path.exists(src_path):
        for root, dirs, files in os.walk(src_path):
            for file in files:
                if file.endswith(('.tsx', '.jsx', '.ts', '.js')):
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            
                            # Procurar por endpoints
                            import re
                            api_patterns = [
                                r'["\'](/api/[^"\']+)["\']',
                                r'fetch\(["\']([^"\']+)["\']',
                                r'axios\.[get|post|put|delete]+\(["\']([^"\']+)["\']'
                            ]
                            
                            for pattern in api_patterns:
                                matches = re.findall(pattern, content)
                                for match in matches:
                                    if match.startswith('/api/'):
                                        endpoints.add(match)
                    except:
                        continue
    
    return sorted(list(endpoints))

async def extract_external_dependencies(project_path: str) -> List[str]:
    """Extrai depend√™ncias externas"""
    
    services = set()
    
    src_path = os.path.join(project_path, "src")
    if os.path.exists(src_path):
        for root, dirs, files in os.walk(src_path):
            for file in files:
                if file.endswith(('.tsx', '.jsx', '.ts', '.js')):
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            
                            # Procurar por servi√ßos externos
                            import re
                            external_patterns = [
                                r'https?://[a-zA-Z0-9.-]+/[a-zA-Z0-9./-]+',
                                r'[a-zA-Z0-9.-]+\.com/[a-zA-Z0-9./-]+'
                            ]
                            
                            for pattern in external_patterns:
                                matches = re.findall(pattern, content)
                                for match in matches:
                                    if 'localhost' not in match and '127.0.0.1' not in match:
                                        services.add(match)
                    except:
                        continue
    
    return sorted(list(services))

async def extract_env_variables(project_path: str) -> List[str]:
    """Extrai vari√°veis de ambiente"""
    
    env_vars = set()
    
    src_path = os.path.join(project_path, "src")
    if os.path.exists(src_path):
        for root, dirs, files in os.walk(src_path):
            for file in files:
                if file.endswith(('.tsx', '.jsx', '.ts', '.js')):
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            
                            # Procurar por vari√°veis de ambiente
                            import re
                            env_patterns = [
                                r'process\.env\.([A-Z_]+)',
                                r'import\.meta\.env\.([A-Z_]+)',
                                r'["\']([A-Z_]+)["\'].*environment'
                            ]
                            
                            for pattern in env_patterns:
                                matches = re.findall(pattern, content)
                                for match in matches:
                                    env_vars.add(match)
                    except:
                        continue
    
    return sorted(list(env_vars))

async def generate_phase_recommendations(validation_score: int, artifacts_info: Dict[str, Any]) -> List[str]:
    """Gera recomenda√ß√µes para a pr√≥xima fase"""
    
    recommendations = []
    
    if validation_score < 75:
        recommendations.append("‚ö†Ô∏è Melhore a qualidade do c√≥digo antes do deploy")
        recommendations.append("üß™ Aumente a cobertura de testes")
    
    if artifacts_info.get("file_types", {}).get("tests", 0) < 5:
        recommendations.append("üìã Adicione mais testes automatizados")
    
    if validation_score >= 75:
        recommendations.append("‚úÖ Projeto pronto para deploy")
        recommendations.append("üöÄ Configure o pipeline de CI/CD")
        recommendations.append("üìä Monitore a aplica√ß√£o em produ√ß√£o")
    
    recommendations.extend([
        "üîß Configure vari√°veis de ambiente de produ√ß√£o",
        "üìù Documente o processo de deploy",
        "üîç Configure monitoramento e alertas"
    ])
    
    return recommendations

async def prepare_production_assets(project_path: str) -> List[Dict[str, Any]]:
    """Prepara assets para produ√ß√£o"""
    
    assets = []
    
    # Otimiza√ß√£o de imagens
    image_optimizer = {
        "type": "asset_optimization",
        "asset_type": "images",
        "config": {
            "format": "webp",
            "quality": 80,
            "lazy_loading": True,
            "responsive_images": True,
            "srcset_generation": True
        }
    }
    assets.append(image_optimizer)
    
    # Minifica√ß√£o de c√≥digo
    minifier = {
        "type": "asset_optimization",
        "asset_type": "code",
        "config": {
            "minify": True,
            "sourcemaps": False,
            "treeshaking": True,
            "dead_code_elimination": True,
            "css_minification": True
        }
    }
    assets.append(minifier)
    
    # Otimiza√ß√£o de bundle
    bundle_optimizer = {
        "type": "asset_optimization",
        "asset_type": "bundle",
        "config": {
            "code_splitting": True,
            "chunk_optimization": True,
            "compression": "gzip",
            "browser_cache": True,
            "cdn_optimization": True
        }
    }
    assets.append(bundle_optimizer)
    
    return assets

async def generate_deploy_scripts(project_path: str, next_phase: str) -> List[Dict[str, Any]]:
    """Gera scripts de deploy"""
    
    scripts = []
    
    # Script de deploy
    deploy_script = {
        "type": "deploy_script",
        "name": "deploy.sh",
        "content": """#!/bin/bash
set -e

echo "üöÄ Iniciando deploy do frontend..."

# Build
echo "üì¶ Buildando aplica√ß√£o..."
npm run build

# Testes
echo "üß™ Executando testes..."
npm run test

# Deploy para staging
if [ "$1" = "staging" ]; then
    echo "üåç Deploy para staging..."
    # Comandos para staging
    echo "‚úÖ Deploy para staging conclu√≠do"
fi

# Deploy para produ√ß√£o
if [ "$1" = "production" ]; then
    echo "üåç Deploy para produ√ß√£o..."
    # Comandos para produ√ß√£o
    echo "‚úÖ Deploy para produ√ß√£o conclu√≠do"
fi

echo "üéâ Deploy conclu√≠do com sucesso!"
"""
    }
    scripts.append(deploy_script)
    
    # Script de rollback
    rollback_script = {
        "type": "deploy_script",
        "name": "rollback.sh",
        "content": """#!/bin/bash
set -e

echo "üîÑ Iniciando rollback..."

# Reverter √∫ltimo commit
git revert HEAD --no-edit

# Build novamente
npm run build

# Deploy
echo "üåç Fazendo deploy da vers√£o anterior..."
# Comandos de deploy

echo "‚úÖ Rollback conclu√≠do com sucesso!"
"""
    }
    scripts.append(rollback_script)
    
    return scripts

async def generate_cicd_config(project_path: str, next_phase: str) -> Dict[str, Any]:
    """Gera configura√ß√£o de CI/CD"""
    
    cicd_config = {
        "type": "cicd_config",
        "platform": "github_actions",
        "config": {
            "workflow_file": ".github/workflows/deploy.yml",
            "triggers": ["push", "pull_request"],
            "jobs": {
                "test": {
                    "runs_on": "ubuntu-latest",
                    "steps": [
                        "checkout",
                        "setup_node",
                        "install_dependencies",
                        "run_tests",
                        "run_lint",
                        "build"
                    ]
                },
                "deploy": {
                    "needs": "test",
                    "runs_on": "ubuntu-latest",
                    "steps": [
                        "checkout",
                        "setup_node",
                        "install_dependencies",
                        "build",
                        "deploy_to_staging",
                        "run_e2e_tests",
                        "deploy_to_production"
                    ]
                }
            }
        }
    }
    
    # Gerar workflow file
    workflow_content = """name: Frontend CI/CD

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
      
      - run: npm ci
      - run: npm run lint
      - run: npm run test
      - run: npm run build
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3

  deploy-staging:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
      
      - run: npm ci
      - run: npm run build
      
      - name: Deploy to staging
        run: |
          # Deploy commands here
          echo "Deploying to staging..."

  deploy-production:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
      
      - run: npm ci
      - run: npm run build
      
      - name: Deploy to production
        run: |
          # Deploy commands here
          echo "Deploying to production..."
"""
    
    workflow_path = os.path.join(project_path, ".github", "workflows", "deploy.yml")
    os.makedirs(os.path.dirname(workflow_path), exist_ok=True)
    
    with open(workflow_path, 'w', encoding='utf-8') as f:
        f.write(workflow_content)
    
    cicd_config["workflow_file_path"] = workflow_path
    return cicd_config

async def generate_quality_report(project_path: str, validation_score: int) -> Dict[str, Any]:
    """Gera relat√≥rio de qualidade"""
    
    report = {
        "type": "quality_report",
        "validation_score": validation_score,
        "metrics": {
            "test_coverage": await estimate_test_coverage(project_path),
            "performance_score": await estimate_performance_score(project_path),
            "accessibility_compliance": await estimate_accessibility_compliance(project_path),
            "security_score": await estimate_security_score(project_path)
        },
        "recommendations": await generate_phase_recommendations(validation_score, {}),
        "generated_at": "2026-01-29T17:00:00Z"
    }
    
    # Salvar relat√≥rio
    report_path = os.path.join(project_path, "docs", "11-deploy", "quality-report.json")
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2)
    
    report["report_file_path"] = report_path
    return report

# Exportar fun√ß√£o principal
__all__ = ['process_frontend_to_next_phase']
